<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="f957a323dec74ac73ed944c20db03f0b11613e4a" /><Meta Name="ms.sourcegitcommit" Value="756d085f27705e86604f1bba5f2086ee23761acf" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="zh-TW" /><Meta Name="ms.lasthandoff" Value="01/30/2019" /><Meta Name="ms.locfileid" Value="55349901" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>提供方法來存取和管理處理中的語音辨識引擎。</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 您可以建立這個類別的執行個體的任何已安裝的語音辨識器。 若要取得辨識器已安裝相關的資訊，請使用靜態<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>方法。  
  
 此類別適用於語音辨識引擎在同處理序執行，並可讓您控制語音辨識的各個層面，如下所示：  
  
-   若要建立同處理序語音辨識器，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A>建構函式。  
  
-   若要管理的語音辨識文法，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>，以及<xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A>方法，而<xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A>屬性。  
  
-   若要設定的辨識器的輸入，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>，或<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>方法。  
  
-   若要執行語音辨識，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法。  
  
-   若要修改辨識如何處理無回應或非預期的輸入，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
-   若要變更辨識器傳回的替代項目數目，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A>屬性。 辨識器傳回在辨識結果<xref:System.Speech.Recognition.RecognitionResult>物件。  
  
-   若要變更同步至辨識器，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>方法。 辨識器會使用一個以上的執行緒來執行工作。  
  
-   若要模擬的辨識器輸入，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>物件是唯一使用的具現化物件的程序。 相較之下，<xref:System.Speech.Recognition.SpeechRecognizer>與想要使用它的任何應用程式共用單一的辨識器。  
  
> [!NOTE]
>  請務必呼叫<xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A>發行您的語音辨識器的最後一個參考之前。 否則，它所使用的資源將不會釋放之前，記憶體回收行程呼叫辨識器物件的`Finalize`方法。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 因為此範例會使用`Multiple`模式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法，它會執行辨識直到您關閉主控台視窗，或停止偵錯。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 類別的新執行個體。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 您可以建構<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體，從下列任一項：  
  
-   系統的預設語音辨識引擎  
  
-   依名稱指定特定的語音辨識引擎  
  
-   如您所指定的地區設定的預設語音辨識引擎  
  
-   符合您在中指定特定的辨識引擎<xref:System.Speech.Recognition.RecognizerInfo>物件。  
  
 語音辨識器開始辨識之前，您必須載入至少一個語音辨識文法，並設定辨識器的輸入。  
  
 若要載入的文法，請呼叫<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
 若要設定之音訊輸入，請使用下列方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>使用系統的預設語音辨識器，初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 類別的新執行個體。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器開始語音辨識之前，您必須載入至少一個辨識文法，並設定辨識器的輸入。  
  
 若要載入的文法，請呼叫<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
 若要設定之音訊輸入，請使用下列方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">語音辨識器必須支援的地區設定。</param>
        <summary>使用指定之地區設定的預設語音辨識器，初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 類別的新執行個體。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows 和 System.Speech API 接受所有有效的語言-國家/地區代碼。 若要執行使用中所指定語言的語音辨識`CultureInfo`引數，支援語言-國家/地區的程式碼必須先安裝的語音辨識引擎。 隨附於 Microsoft Windows 7 的語音辨識引擎會使用下列的語言-國家/地區代碼。  
  
-   en GB。 English (United Kingdom)  
  
-   EN-US。 英文 （美國）  
  
-   de-德國。 德文 （德國）  
  
-   es-ES。 西班牙文 （西班牙）  
  
-   fr-fr。 法文 （法國）  
  
-   若為 JA-JP。 日文 （日本）  
  
-   zh-CN。 中文 （中國）  
  
-   zh-TW。 中文 （台灣）  
  
 兩個字母的語言代碼，例如"en"，"fr"，或也允許"es"。  
  
 語音辨識器開始辨識之前，您必須載入至少一個語音辨識文法，並設定辨識器的輸入。  
  
 若要載入的文法，請呼叫<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
 若要設定之音訊輸入，請使用下列方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下列範例示範基本的語音辨識，並初始化 EN-US 地區設定的語音辨識器的主控台應用程式的一部分。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">已安裝的語音辨識器都不支援所指定的地區設定，或者 <paramref name="culture" /> 是不因文化特性而異 (Invariant Culture)。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" /> 為 <see langword="null" />。</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">特定語音辨識器的資訊。</param>
        <summary>使用 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 物件中的資訊指定要使用的辨識器，初始化 <see cref="T:System.Speech.Recognition.RecognizerInfo" /> 的新執行個體。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 您可以建立這個類別的執行個體的任何已安裝的語音辨識器。 若要取得辨識器已安裝相關的資訊，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>方法。  
  
 語音辨識器開始辨識之前，您必須載入至少一個語音辨識文法，並設定辨識器的輸入。  
  
 若要載入的文法，請呼叫<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
 若要設定之音訊輸入，請使用下列方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下列範例示範基本的語音辨識，並初始化支援英文語言的語音辨識器的主控台應用程式的一部分。  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">要使用語音辨識器的權杖名稱。</param>
        <summary>使用字串參數指定要使用的辨識器名稱，初始化 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 類別的新執行個體。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 辨識器的語彙基元的名稱是值<xref:System.Speech.Recognition.RecognizerInfo.Id%2A>的屬性<xref:System.Speech.Recognition.RecognizerInfo>所傳回的物件<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A>辨識器的屬性。 若要取得所有已安裝的辨識器的集合，使用靜態<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>方法。  
  
 語音辨識器開始辨識之前，您必須載入至少一個語音辨識文法，並設定辨識器的輸入。  
  
 若要載入的文法，請呼叫<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
 若要設定之音訊輸入，請使用下列方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 下列範例示範的主控台應用程式，示範基本的語音辨識，以及建立 Windows 語音辨識器 8.0 的執行個體的組件 (英文-美國)。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">沒有安裝該語彙基元名稱的語音辨識器，或者 <paramref name="recognizerId" /> 是空字串 ("")。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" /> 為 <see langword="null" />。</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 所接收的音訊格式。</summary>
        <value>輸入至 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 執行個體時的音訊格式，如果輸入未設定或設為 null 輸入則為 <see langword="null" />。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要設定之音訊輸入，請使用下列方法之一：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 以下示例使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A>來獲取並顯示音頻格式數據。  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 所接收的音訊層級。</summary>
        <value>語音辨識器的輸入音量，大小從 0 到 100。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 值 0 表示無回應，以及 100 表示最大輸入磁碟區。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 報告其音訊輸入層級時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>引發這個事件每秒數次。 用來引發事件的頻率取決於應用程式執行所在的電腦。  
  
 若要取得的音訊層級事件時，請使用<xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A>相關聯的屬性<xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>。 若要取得目前的辨識器的輸入音訊層級，使用辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A>屬性。  
  
 當您建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> 委派 (Delegate) 時，就可以識別即將處理此事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例會加入處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>事件，以<xref:System.Speech.Recognition.SpeechRecognitionEngine>物件。 處理常式會輸出新的音訊層級寫入主控台。  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得提供 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 輸入的裝置正在產生的音訊資料流中目前的位置。</summary>
        <value>輸入裝置所產生之音訊資料流中的目前位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>屬性會參考其產生的音訊資料流中的輸入的裝置的位置。 相較之下，<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>屬性會參考其音訊輸入中的辨識器的位置。 這些位置可能會不同。 比方說，如果辨識器已收到輸入其已不但產生的值就辨識結果<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>屬性是值小於<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>屬性。  
  
   
  
## Examples  
 在下列範例中，同處理序語音辨識器會使用聽寫文法，若要比對語音輸入。 處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>事件會寫入主控台<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A>當語音辨識器偵測到在其輸入的語音。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>在 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 偵測到音訊訊號問題時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要取得發生的問題，請使用<xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A>相關聯的屬性<xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>。  
  
 當您建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> 委派 (Delegate) 時，就可以識別即將處理此事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例會定義需收集資訊的事件處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>事件。  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 所接收的音訊狀態。</summary>
        <value>語音辨識器之音訊輸入的狀態。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>屬性表示的成員的音訊狀態<xref:System.Speech.Recognition.AudioState>列舉型別。  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>在 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 所接收的音訊變更狀態時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要取得的音訊狀態事件時，請使用<xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A>相關聯的屬性<xref:System.Speech.Recognition.AudioStateChangedEventArgs>。 取得辨識器的輸入音訊的目前狀態，請使用辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>屬性。 如需音訊狀態的詳細資訊，請參閱<xref:System.Speech.Recognition.AudioState>列舉型別。  
  
 當您建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> 委派 (Delegate) 時，就可以識別即將處理此事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例使用的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>要寫入辨識器事件的新<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>主控台每次變更，使用的成員<xref:System.Speech.Recognition.AudioState>列舉型別。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得或設定完成辨識前 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 接受只包含背景雜訊之輸入的時間間隔。</summary>
        <value>時間間隔的持續時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每個語音辨識器有演算法，可區別無回應和語音。 辨識器會分類為任何非無回應可讓您輸入不符合初始規則，以辨識器的任何背景噪音載入，並啟用語音辨識文法。 如果辨識器收到 babble 逾時間隔內時，只有背景雜音及無回應，然後在辨識器完成該辨識作業。  
  
-   非同步辨識作業的辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，其中<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType>屬性是`true`，而<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType>屬性是`null`。  
  
-   針對同步的辨識作業和模擬，辨識器會傳回`null`，而不是有效<xref:System.Speech.Recognition.RecognitionResult>。  
  
 Babble 逾時期限設為 0 時，如果辨識器不會執行 babble 逾時核取。 逾時間隔可以是任何非負數值。 預設值為 0 秒。  
  
   
  
## Examples  
 下列範例示範如何設定基本的語音辨識的主控台應用程式的一部分<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>並<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性的<xref:System.Speech.Recognition.SpeechRecognitionEngine>初始化語音辨識之前。 語音辨識器的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>並<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件輸出事件資訊至主控台，以示範如何<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>的屬性<xref:System.Speech.Recognition.SpeechRecognitionEngine>影響辨識作業。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">這個屬性設定為小於 0 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>處置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 物件。</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>處置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 物件。</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing"><see langword="true" /> 表示會同時釋放 Managed 和 Unmanaged 資源，<see langword="false" /> 則表示只釋放 Unmanaged 資源。</param>
        <summary>處置 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 物件，並釋放工作階段期間所使用的資源。</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>在同步語音辨識中使用文字代替音訊，模擬對語音辨識器的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這些方法會略過系統音訊輸入，並提供做為辨識器的文字<xref:System.String>物件或陣列的<xref:System.Speech.Recognition.RecognizedWordUnit>物件。 當您正在測試或偵錯的應用程式或文法，這可以是很有幫助。 例如，您可以使用模擬來判斷某個字是否文法中，以及辨識單字後，會傳回哪些語意。 使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>方法，以模擬作業期間停用語音辨識引擎的音訊輸入。  
  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。 辨識器會忽略新行和額外的空白字元和標點符號視為常值的輸入。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult>模擬輸入的回應中的語音辨識器所產生的物件具有值`null`針對其<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>屬性。  
  
 若要模擬非同步辨識，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">辨識作業的輸出。</param>
        <summary>在共用的語音辨識器上模擬片語輸入，針對同步的語音辨識使用文字來代替音訊。</summary>
        <returns>辨識作業的結果，如果作業未成功或辨識器未啟用則為 <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。  
  
 Vista 和 Windows 7 隨附的辨識器會忽略大小寫，並將文法規則套用至輸入的片語時，字元寬度。 如需這種類型的比較的詳細資訊，請參閱<xref:System.Globalization.CompareOptions>列舉值<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>和<xref:System.Globalization.CompareOptions.IgnoreWidth>。 辨識器也會忽略新行，以及額外的空白字元和標點符號視為常值的輸入。  
  
   
  
## Examples  
 下列程式碼範例是示範模擬的輸入、 相關聯的辨識結果，以及語音辨識器所引發的相關聯的事件的主控台應用程式的一部分。 這個範例會產生下列輸出。  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">辨識器沒有載入語音辨識文法。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 為空字串 ("")。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">文字單位陣列，包含辨識作業的輸入。</param>
        <param name="compareOptions">列舉值的位元組合，描述要用於模擬辨識作業的比較類型。</param>
        <summary>在語音辨識器上模擬特定單字輸入，針對同步的語音辨識使用文字取代音訊，並指定辨識器如何處理單字間的 Unicode 比較以及已載入的語音辨識文法。</summary>
        <returns>辨識作業的結果，如果作業未成功或辨識器未啟用則為 <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。  
  
 辨識器使用`compareOptions`當它適用於文法規則進行剖析的輸入片語。 Vista 和 Windows 7 隨附的辨識器忽略大小寫，如果<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>或<xref:System.Globalization.CompareOptions.IgnoreCase>值。 辨識器一律會忽略字元寬度，並永遠不會忽略假名類型。 辨識器也會忽略新行和額外的空白字元和標點符號視為常值的輸入。 如需有關字元寬度] 和 [假名類型的詳細資訊，請參閱<xref:System.Globalization.CompareOptions>列舉型別。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">辨識器沒有載入語音辨識文法。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> 包含一個或多個 <see langword="null" /> 項目。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 旗標。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">辨識作業的輸入片語。</param>
        <param name="compareOptions">列舉值的位元組合，描述要用於模擬辨識作業的比較類型。</param>
        <summary>在語音辨識器上模擬片語輸入，針對同步的語音辨識使用文字來代替音訊，並指定辨識器如何處理片語間的 Unicode 比較以及已載入的語音辨識文法。</summary>
        <returns>辨識作業的結果，如果作業未成功或辨識器未啟用則為 <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。  
  
 辨識器使用`compareOptions`當它適用於文法規則進行剖析的輸入片語。 Vista 和 Windows 7 隨附的辨識器忽略大小寫，如果<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>或<xref:System.Globalization.CompareOptions.IgnoreCase>值。 辨識器一律會忽略字元寬度，並永遠不會忽略假名類型。 辨識器也會忽略新行和額外的空白字元和標點符號視為常值的輸入。 如需有關字元寬度] 和 [假名類型的詳細資訊，請參閱<xref:System.Globalization.CompareOptions>列舉型別。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">辨識器沒有載入語音辨識文法。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 為空字串 ("")。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 旗標。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>在非同步語音辨識中使用文字代替音訊，模擬對語音辨識器的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這些方法會略過系統音訊輸入，並提供做為辨識器的文字<xref:System.String>物件或陣列的<xref:System.Speech.Recognition.RecognizedWordUnit>物件。 當您正在測試或偵錯的應用程式或文法，這可以是很有幫助。 例如，您可以使用模擬來判斷某個字是否文法中，以及辨識單字後，會傳回哪些語意。 使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>方法，以模擬作業期間停用語音辨識引擎的音訊輸入。  
  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。 當辨識器完成非同步辨識作業時，會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件。 辨識器會忽略新行和額外的空白字元和標點符號視為常值的輸入。  
  
> [!NOTE]
>  <xref:System.Speech.Recognition.RecognitionResult>模擬輸入的回應中的語音辨識器所產生的物件具有值`null`針對其<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>屬性。  
  
 若要模擬同步辨識，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">辨識作業的輸出。</param>
        <summary>在語音辨識器上模擬片語輸入，針對非同步的語音辨識使用文字來代替音訊。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。 當辨識器完成非同步辨識作業時，會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件。  
  
 Vista 和 Windows 7 隨附的辨識器會忽略大小寫，並將文法規則套用至輸入的片語時，字元寬度。 如需這種類型的比較的詳細資訊，請參閱<xref:System.Globalization.CompareOptions>列舉值<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>和<xref:System.Globalization.CompareOptions.IgnoreWidth>。 辨識器也會忽略新行，以及額外的空白字元和標點符號視為常值的輸入。  
  
   
  
## Examples  
 下列程式碼範例是示範非同步模擬的輸入、 相關聯的辨識結果，以及語音辨識器所引發的相關聯的事件的主控台應用程式的一部分。 這個範例會產生下列輸出。  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">辨識器沒有載入語音辨識文法，或辨識器有尚未完成的非同步辨識作業。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 為空字串 ("")。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">文字單位陣列，包含辨識作業的輸入。</param>
        <param name="compareOptions">列舉值的位元組合，描述要用於模擬辨識作業的比較類型。</param>
        <summary>在語音辨識器上模擬特定單字輸入，針對非同步的語音辨識使用 <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> 物件陣列以取代音訊，並指定辨識器如何處理單字間的 Unicode 比較以及已載入的語音辨識文法。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。 當辨識器完成非同步辨識作業時，會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件。  
  
 辨識器使用`compareOptions`當它適用於文法規則進行剖析的輸入片語。 Vista 和 Windows 7 隨附的辨識器忽略大小寫，如果<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>或<xref:System.Globalization.CompareOptions.IgnoreCase>值。 辨識器一律忽略字元寬度，並永遠不會忽略假名類型。 辨識器也會忽略新行，以及額外的空白字元和標點符號視為常值的輸入。 如需有關字元寬度] 和 [假名類型的詳細資訊，請參閱<xref:System.Globalization.CompareOptions>列舉型別。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">辨識器沒有載入語音辨識文法，或辨識器有尚未完成的非同步辨識作業。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> 包含一個或多個 <see langword="null" /> 項目。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 旗標。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">辨識作業的輸入片語。</param>
        <param name="compareOptions">列舉值的位元組合，描述要用於模擬辨識作業的比較類型。</param>
        <summary>在語音辨識器上模擬片語輸入，針對非同步的語音辨識使用文字來代替音訊，並指定辨識器如何處理片語間的 Unicode 比較以及已載入的語音辨識文法。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件彷彿未模擬辨識作業。 當辨識器完成非同步辨識作業時，會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件。  
  
 辨識器使用`compareOptions`當它適用於文法規則進行剖析的輸入片語。 Vista 和 Windows 7 隨附的辨識器忽略大小寫，如果<xref:System.Globalization.CompareOptions.OrdinalIgnoreCase>或<xref:System.Globalization.CompareOptions.IgnoreCase>值。 辨識器一律忽略字元寬度，並永遠不會忽略假名類型。 辨識器也會忽略新行，以及額外的空白字元和標點符號視為常值的輸入。 如需有關字元寬度] 和 [假名類型的詳細資訊，請參閱<xref:System.Globalization.CompareOptions>列舉型別。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">辨識器沒有載入語音辨識文法，或辨識器有尚未完成的非同步辨識作業。</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> 為空字串 ("")。</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> 包含 <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />、<see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> 或 <see cref="F:System.Globalization.CompareOptions.StringSort" /> 旗標。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 完成模擬輸入的非同步辨識作業時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每個<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法開始非同步辨識作業。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>它結束非同步作業的事件。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>作業可能會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件是最後一次該辨識器會針對指定的作業會引發這類事件。  
  
 如果模擬的辨識成功，您可以存取的辨識結果，使用下列其中一項：  
  
-   <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A>中的屬性<xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>中的處理常式物件<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件。  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> 中的屬性<xref:System.Speech.Recognition.SpeechRecognizedEventArgs>中的處理常式物件<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。  
  
 如果不成功，模擬的辨識<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>不會引發事件和<xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A>將會是 null。  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> 是衍生自 <xref:System.ComponentModel.AsyncCompletedEventArgs>。  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> 是衍生自 <xref:System.Speech.Recognition.RecognitionEventArgs>。  
  
 當您建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> 委派 (Delegate) 時，就可以識別即將處理此事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例是主控台應用程式載入語音辨識文法，並示範非同步模擬的輸入、 相關聯的辨識結果，以及語音辨識器所引發的相關聯的事件的一部分。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得或設定靜默無聲間隔，<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 會在意義明確輸入的結尾接受這段間隔的無聲輸入，然後才完成辨識作業。</summary>
        <value>無回應間隔的持續時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器辨識輸入模稜兩可時，會使用此逾時間隔。 例如語音辨識文法支援的辨識 」 新遊戲請 」 或 「 新遊戲"，"新遊戲請 」 是模稜兩可的輸入，而 「 新遊戲 」 是模稜兩可的輸入。  
  
 此屬性決定的語音辨識引擎將會等候的時間額外的輸入然後才完成辨識作業。 逾時間隔可以是 0 秒到 10 秒，內含。 預設值為 150 （毫秒）。  
  
 若要設定的逾時間隔，模稜兩可的輸入，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">這個屬性已設定為小於 0 秒或大於 10 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得或設定靜默無聲間隔，<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 會在模稜兩可輸入的結尾接受這段間隔的無聲輸入，然後才完成辨識作業。</summary>
        <value>無回應間隔的持續時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識器辨識輸入模稜兩可時，會使用此逾時間隔。 例如語音辨識文法支援的辨識 」 新遊戲請 」 或 「 新遊戲"，"新遊戲請 」 是模稜兩可的輸入，而 「 新遊戲 」 是模稜兩可的輸入。  
  
 此屬性決定的語音辨識引擎將會等候的時間額外的輸入然後才完成辨識作業。 逾時間隔可以是 0 秒到 10 秒，內含。 預設值為 500 毫秒。  
  
 若要設定的逾時間隔，模稜兩可的輸入，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>屬性。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">這個屬性已設定為小於 0 秒或大於 10 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得在這個 <see cref="T:System.Speech.Recognition.Grammar" /> 執行個體中載入之 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 物件的集合。</summary>
        <value><see cref="T:System.Speech.Recognition.Grammar" /> 物件的集合。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下列範例會輸出到主控台以供每個目前已載入語音辨識器的語音辨識文法的資訊。  
  
> [!IMPORTANT]
>  複製文法集合，以避免發生錯誤，若集合已經過修改，而這個方法會列舉集合的元素。  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得或設定完成辨識前 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 接受只包含靜音之輸入的時間間隔。</summary>
        <value>無回應間隔的持續時間。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每個語音辨識器有演算法，可區別無回應和語音。 如果辨識器輸入的初始靜音逾時期間是無回應，然後在辨識器完成該辨識作業。  
  
-   辨識器所引發的非同步辨識作業，模擬<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，其中<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType>屬性是`true`，而<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType>屬性是`null`。  
  
-   針對同步的辨識作業和模擬，辨識器會傳回`null`，而不是有效<xref:System.Speech.Recognition.RecognitionResult>。  
  
 初始靜音逾時間隔會設為 0，如果辨識器不會執行的初始靜音逾時檢查。 逾時間隔可以是任何非負數值。 預設值為 0 秒。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 範例會設定<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>並<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性的<xref:System.Speech.Recognition.SpeechRecognitionEngine>初始化語音辨識之前。 語音辨識器的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>並<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件輸出事件資訊至主控台，以示範如何<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>的屬性<xref:System.Speech.Recognition.SpeechRecognitionEngine>屬性會影響辨識作業。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">這個屬性設定為小於 0 秒。</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>傳回目前的系統上所有已安裝的語音辨識器的資訊。</summary>
        <returns><see cref="T:System.Speech.Recognition.RecognizerInfo" /> 物件的唯讀集合，這些物件會描述已安裝的辨識器。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要取得目前的辨識器的相關資訊，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A>屬性。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 此範例會使用所傳回的集合<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>方法來尋找支援英文語言的語音辨識器。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">要載入的文法物件。</param>
        <summary>同步載入 <see cref="T:System.Speech.Recognition.Grammar" /> 物件。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果辨識器擲回例外狀況<xref:System.Speech.Recognition.Grammar>物件已經載入，以非同步方式載入時，或無法載入任何辨識器。 您無法載入相同<xref:System.Speech.Recognition.Grammar>成多個執行個體的物件<xref:System.Speech.Recognition.SpeechRecognitionEngine>。 相反地，建立新<xref:System.Speech.Recognition.Grammar>物件給每個<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體。  
  
 如果辨識器正在執行，應用程式必須使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>暫停之前載入、 卸載、 啟用，或停用文法的語音辨識引擎。  
  
 當您載入的文法時，預設會啟用它。 若要停用已載入的文法，請使用<xref:System.Speech.Recognition.Grammar.Enabled%2A>屬性。  
  
 若要載入<xref:System.Speech.Recognition.Grammar>物件以非同步方式，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 此範例會建立<xref:System.Speech.Recognition.DictationGrammar>並將它載入語音辨識器。  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> 不是有效狀態。</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">要載入的語音辨識文法。</param>
        <summary>以非同步方式載入語音辨識文法。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 當辨識器完成載入時<xref:System.Speech.Recognition.Grammar>物件，便會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>事件。 如果辨識器擲回例外狀況<xref:System.Speech.Recognition.Grammar>物件已經載入，以非同步方式載入時，或無法載入任何辨識器。 您無法載入相同<xref:System.Speech.Recognition.Grammar>成多個執行個體的物件<xref:System.Speech.Recognition.SpeechRecognitionEngine>。 相反地，建立新<xref:System.Speech.Recognition.Grammar>物件給每個<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體。  
  
 如果辨識器正在執行，應用程式必須使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>暫停之前載入、 卸載、 啟用，或停用文法的語音辨識引擎。  
  
 當您載入的文法時，預設會啟用它。 若要停用已載入的文法，請使用<xref:System.Speech.Recognition.Grammar.Enabled%2A>屬性。  
  
 若要以同步方式載入語音辨識文法，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>方法。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.InvalidOperationException"><paramref name="Grammar" /> 不是有效狀態。</exception>
        <exception cref="T:System.OperationCanceledException">非同步作業已取消。</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 完成 <see cref="T:System.Speech.Recognition.Grammar" /> 物件的非同步載入時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法來啟始非同步作業。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>引發這個事件時完成此作業。 若要取得<xref:System.Speech.Recognition.Grammar>物件辨識器載入，請使用<xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A>相關聯的屬性<xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>。 若要取得目前<xref:System.Speech.Recognition.Grammar>辨識器已載入，物件會使用辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A>屬性。  
  
 如果辨識器正在執行，應用程式必須使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>暫停之前載入、 卸載、 啟用，或停用文法的語音辨識引擎。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例會建立同處理序的語音辨識器，並接著會建立兩種類型的特定單字的辨識和接受免費的聽寫文法。 此範例會建構<xref:System.Speech.Recognition.Grammar>物件從每個已完成的語音辨識文法，然後以非同步方式載入<xref:System.Speech.Recognition.Grammar>物件到<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體。 辨識器的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>並<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件都會寫入主控台的名稱<xref:System.Speech.Recognition.Grammar>用於分別執行辨識和文字辨識結果的物件。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得或設定 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 為每個辨識作業傳回之替代辨識結果的最大數目。</summary>
        <value>要傳回的替代結果數目。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>的屬性<xref:System.Speech.Recognition.RecognitionResult>類別包含的集合<xref:System.Speech.Recognition.RecognizedPhrase>代表可能的解譯，輸入的物件。  
  
 預設值為<xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A>為 10。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException"><see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> 設定為小於 0 的值。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">要傳回之設定的名稱。</param>
        <summary>傳回辨識器的設定值。</summary>
        <returns>設定的值。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 辨識器的設定可以包含字串、 64 位元整數或記憶體位址資料。 下表描述所定義的 Microsoft Speech API (SAPI) 設定為符合規範的辨識器。 下列設定必須有相同的範圍，支援設定每個辨識器。 SAPI 相容的辨識器不需要支援這些設定，而且可支援其他設定。  
  
|名稱|描述|  
|----------|-----------------|  
|`ResourceUsage`|指定辨識器的 CPU 耗用量。 範圍是從 0 到 100 之間。 預設值為 50。|  
|`ResponseSpeed`|語音辨識器完成辨識作業之前，請指定模稜兩可的輸入結尾處的無回應的長度。 範圍是從 0 到 10000 毫秒 (ms)。 此設定會對應至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>屬性。  預設 = 150ms年。|  
|`ComplexResponseSpeed`|語音辨識器完成辨識作業之前，請指定模稜兩可的輸入結尾處的無回應的長度。 範圍是從 0 到 10,000。 此設定會對應至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。 預設 = 500 毫秒。|  
|`AdaptationOn`|指出調適的原音模型是否為 ON (值 = `1`) 或 OFF (值 = `0`)。 預設值是`1`(ON)。|  
|`PersistedBackgroundAdaptation`|指出背景適應是否為 ON (值 = `1`) 或 OFF (值 = `0`)，並保存在登錄中的設定。 預設值是`1`(ON)。|  
  
 若要更新辨識器的設定，請使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>方法。  
  
   
  
## Examples  
 下列範例是主控台應用程式，讓它輸出幾個的辨識器支援 EN-US 地區設定所定義的設定值的一部分。 這個範例會產生下列輸出。  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> 為空字串 ("")。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">辨識器沒有該名稱的設定。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>啟動同步語音辨識作業。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這些方法會執行單一、 同步辨識作業。 辨識器會執行這項作業對其載入且已啟用的語音辨識文法。  
  
 在這個方法呼叫，辨識器可能會引發下列事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  當辨識器偵測到可辨識為語音的輸入時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  輸入以其中一個作用中的文法建立模稜兩可的符合項目時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 當辨識器完成辨識作業時引發。  
  
 辨識器不會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件時使用的其中一個<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>方法。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>方法會傳回<xref:System.Speech.Recognition.RecognitionResult>物件，或`null`如果作業未成功或辨識器未啟用。  
  
 同步的辨識作業可能會失敗，原因如下：  
  
-   逾時間隔到期的前未偵測到語音<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>或是<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性，或針對`initialSilenceTimeout`參數<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>方法。  
  
-   偵測到語音辨識引擎，但沒有找到配對中任何其已載入且已啟用<xref:System.Speech.Recognition.Grammar>物件。  
  
 若要修改的辨識器如何處理語音或辨識方面的無回應的時間，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>必須至少一個<xref:System.Speech.Recognition.Grammar>之前執行辨識載入的物件。 若要載入的語音辨識文法，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
 若要執行非同步辨識，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>執行同步語音辨識作業。</summary>
        <returns>輸入的辨識結果，如果作業未成功或辨識器未啟用則為 <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這個方法會執行單一的辨識作業。 辨識器會執行這項作業對其載入且已啟用的語音辨識文法。  
  
 在這個方法呼叫，辨識器可能會引發下列事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  當辨識器偵測到可辨識為語音的輸入時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  輸入以其中一個作用中的文法建立模稜兩可的符合項目時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 當辨識器完成辨識作業時引發。  
  
 辨識器不會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件時使用此方法。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>方法會傳回<xref:System.Speech.Recognition.RecognitionResult>物件，或`null`如果作業未順利完成。  
  
 同步的辨識作業可能會失敗，原因如下：  
  
-   逾時間隔到期的前未偵測到語音<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性。  
  
-   偵測到語音辨識引擎，但沒有找到配對中任何其已載入且已啟用<xref:System.Speech.Recognition.Grammar>物件。  
  
 若要執行非同步辨識，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 此範例會建立<xref:System.Speech.Recognition.DictationGrammar>、 將其載入至同處理序的語音辨識器，並執行一個辨識作業。  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">語音辨識器在完成辨識前接受僅含靜默無聲之輸入的時間間隔。</param>
        <summary>使用指定的初始靜音逾時期限，執行同步語音辨識作業。</summary>
        <returns>輸入的辨識結果，如果作業未成功或辨識器未啟用則為 <see langword="null" />。</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 語音辨識引擎會在所指定的時間間隔內偵測語音`initialSilenceTimeout`引數，<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29>執行單一的辨識作業，並終止。  `initialSilenceTimeout`參數取代的辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性。  
  
 在這個方法呼叫，辨識器可能會引發下列事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  當辨識器偵測到可辨識為語音的輸入時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  輸入以其中一個作用中的文法建立模稜兩可的符合項目時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 當辨識器完成辨識作業時引發。  
  
 辨識器不會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件時使用此方法。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>方法會傳回<xref:System.Speech.Recognition.RecognitionResult>物件，或`null`如果作業未順利完成。  
  
 同步的辨識作業可能會失敗，原因如下：  
  
-   逾時間隔到期的前未偵測到語音<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>或`initialSilenceTimeout`參數。  
  
-   偵測到語音辨識引擎，但沒有找到配對中任何其已載入且已啟用<xref:System.Speech.Recognition.Grammar>物件。  
  
 若要執行非同步辨識，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 此範例會建立<xref:System.Speech.Recognition.DictationGrammar>、 將其載入至同處理序的語音辨識器，並執行一個辨識作業。  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>啟動非同步語音辨識作業。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這些方法會執行單一或多個非同步辨識作業。 辨識器會執行每個作業，針對其載入且已啟用的語音辨識文法。  
  
 在這個方法呼叫，辨識器可能會引發下列事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  當辨識器偵測到可辨識為語音的輸入時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  輸入以其中一個作用中的文法建立模稜兩可的符合項目時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 當辨識器完成辨識作業時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 時引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>作業完成。  
  
 若要擷取的非同步辨識作業的結果，請將事件處理常式附加至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。 辨識器引發這個事件，每當它成功完成同步或非同步辨識作業。 如果不成功，辨識<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>上的屬性<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>物件，您可以存取的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，將會`null`。  
  
 非同步辨識作業可能會失敗，原因如下：  
  
-   逾時間隔到期的前未偵測到語音<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性。  
  
-   偵測到語音辨識引擎，但沒有找到配對中任何其已載入且已啟用<xref:System.Speech.Recognition.Grammar>物件。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine>必須至少一個<xref:System.Speech.Recognition.Grammar>之前執行辨識載入的物件。 若要載入的語音辨識文法，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>方法。  
  
-   若要修改的辨識器如何處理語音或辨識方面的無回應的時間，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
-   若要執行同步的辨識，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>方法。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>執行單一、非同步語音辨識作業。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這個方法會執行單一、 非同步辨識作業。 辨識器執行的作業對其載入且已啟用的語音辨識文法。  
  
 在這個方法呼叫，辨識器可能會引發下列事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  當辨識器偵測到可辨識為語音的輸入時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  輸入以其中一個作用中的文法建立模稜兩可的符合項目時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 當辨識器完成辨識作業時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 時引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>作業完成。  
  
 若要擷取的非同步辨識作業的結果，請將事件處理常式附加至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。 辨識器引發這個事件，每當它成功完成同步或非同步辨識作業。 如果不成功，辨識<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>上的屬性<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>物件，您可以存取的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，將會`null`。  
  
 若要執行同步的辨識，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>方法。  
  
   
  
## Examples  
 下列範例示範基本的非同步語音辨識的主控台應用程式的一部分。 此範例會建立<xref:System.Speech.Recognition.DictationGrammar>、 將其載入至同處理序的語音辨識器，並執行一項非同步辨識作業。 事件處理常式的以示範辨識器會在作業期間引發的事件。  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">指出是否要執行一或多個辨識作業。</param>
        <summary>執行一項或多項非同步語音辨識作業。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果`mode`已<xref:System.Speech.Recognition.RecognizeMode.Multiple>，在辨識器會繼續執行直到非同步辨識作業<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>呼叫方法。  
  
 在這個方法呼叫，辨識器可能會引發下列事件：  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  當辨識器偵測到可辨識為語音的輸入時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  輸入以其中一個作用中的文法建立模稜兩可的符合項目時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 或 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>。 當辨識器完成辨識作業時引發。  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. 時引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>作業完成。  
  
 若要擷取的非同步辨識作業的結果，請將事件處理常式附加至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。 辨識器引發這個事件，每當它成功完成同步或非同步辨識作業。 如果不成功，辨識<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A>上的屬性<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>物件，您可以存取的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，將會`null`。  
  
 非同步辨識作業可能會失敗，原因如下：  
  
-   逾時間隔到期的前未偵測到語音<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>屬性。  
  
-   偵測到語音辨識引擎，但沒有找到配對中任何其已載入且已啟用<xref:System.Speech.Recognition.Grammar>物件。  
  
 若要執行同步的辨識，使用其中一種<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>方法。  
  
   
  
## Examples  
 下列範例示範基本的非同步語音辨識的主控台應用程式的一部分。 此範例會建立<xref:System.Speech.Recognition.DictationGrammar>、 將其載入至同處理序的語音辨識器，並執行多個非同步辨識作業。 在 30 秒後取消非同步作業。 事件處理常式的以示範辨識器會在作業期間引發的事件。  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>結束非同步辨識，而不等待目前辨識作業完成。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這個方法會立即完成非同步辨識。 如果目前的非同步辨識作業會接收輸入，輸入會被截斷，並在作業完成的現有輸入。 辨識器引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>或是<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件時的非同步作業已取消，並設定<xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A>屬性<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>到`true`。 這個方法會取消非同步作業所起始<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法。  
  
 若要停止非同步辨識，而不截斷輸入，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>方法。  
  
   
  
## Examples  
 下列範例會示範如何使用主控台應用程式的一部分<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>方法。 範例會建立和載入語音辨識文法，啟始繼續的非同步辨識作業，然後暫停 2 秒，就會取消作業之前的資料。 辨識器從檔案中，接收輸入 c:\temp\audioinput\sample.wav。 事件處理常式的以示範辨識器會在作業期間引發的事件。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>在目前的辨識作業完成後，停止非同步辨識。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 這個方法會完成非同步辨識，而不截斷輸入。 如果目前的非同步辨識作業會接收輸入，辨識器會繼續接受輸入，直到目前的辨識作業完成。 辨識器引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>或是<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>事件時的非同步作業已停止，並設定<xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A>屬性<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>到`true`。 此方法會停止所起始的非同步作業<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法。  
  
 若要立即取消非同步辨識與現有的輸入，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A>方法。  
  
   
  
## Examples  
 下列範例會示範如何使用主控台應用程式的一部分<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A>方法。 範例會建立和載入語音辨識文法，啟始繼續的非同步辨識作業，然後暫停 2 秒，就會停止作業之前的資料。 辨識器從檔案中，接收輸入 c:\temp\audioinput\sample.wav。 事件處理常式的以示範辨識器會在作業期間引發的事件。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary><see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 完成非同步辨識作業時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>物件的<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法啟始的非同步辨識作業。 當辨識器結束非同步作業時，它就會引發這個事件。  
  
 使用的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，您可以存取<xref:System.Speech.Recognition.RecognitionResult>在<xref:System.Speech.Recognition.RecognizeCompletedEventArgs>物件。 如果不成功，辨識<xref:System.Speech.Recognition.RecognitionResult>會`null`。 若要判斷是否發生逾時或中斷音訊輸入中的造成失敗的辨識，您可以存取的屬性<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>，或<xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>。  
  
 如需詳細資訊，請參閱 <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> 類別。  
  
 若要取得最佳的拒絕的辨識候選項的詳細資訊，請將附加的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>事件。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例會辨識片語這類 「 爵士的類別目錄中顯示演出者的清單 」 或 「 顯示專輯 gospel 」。 此範例會使用的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>事件，以在主控台中顯示的辨識結果的相關資訊。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 正在處理的音訊輸入的目前位置。</summary>
        <value>辨識器在其所處理之音訊輸入中的位置。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 音訊的位置是針對每個語音辨識器。 啟用時，會建立輸入資料流的零值。  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>屬性參考<xref:System.Speech.Recognition.SpeechRecognitionEngine>其音訊輸入中的物件的位置。 相較之下，<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>屬性會參考其產生的音訊資料流中的輸入的裝置的位置。 這些位置可能會不同。 比方說，如果辨識器已收到輸入其已不但產生的值就辨識結果<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>屬性是值小於<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>屬性。  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>取得關於目前 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 執行個體的資訊。</summary>
        <value>目前語音辨識器的相關資訊。</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 若要取得目前的系統中的所有已安裝的語音辨識器的相關資訊，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A>方法。  
  
   
  
## Examples  
 下列範例會取得資料目前的處理中的語音辨識引擎的部分清單。 如需詳細資訊，請參閱<xref:System.Speech.Recognition.RecognizerInfo>。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>當執行中的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 暫停以接受修改時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 應用程式必須使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>若要暫停的執行個體<xref:System.Speech.Recognition.SpeechRecognitionEngine>之前修改其設定或其<xref:System.Speech.Recognition.Grammar>物件。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>時，引發這個事件已準備好接受修改。  
  
 例如，當<xref:System.Speech.Recognition.SpeechRecognitionEngine>會暫停，您可以載入、 卸載、 啟用和停用<xref:System.Speech.Recognition.Grammar>物件，並修改值<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>屬性。 如需詳細資訊，請參閱 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> 方法。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例示範的主控台應用程式載入和卸載<xref:System.Speech.Recognition.Grammar>物件。 應用程式使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>方法來要求語音辨識引擎暫停讓它可以接收更新。 應用程式，然後載入或卸載<xref:System.Speech.Recognition.Grammar>物件。  
  
 在每個更新的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件會寫入目前載入的狀態與名稱<xref:System.Speech.Recognition.Grammar>至主控台的物件。 當載入和卸載的文法，應用程式第一次會辨識陣列動物的名稱和伺服器陣列的動物名稱的水果、 名稱然後水果的名稱。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>要求辨識器暫停以更新其狀態。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 使用這個方法將變更同步至辨識器。 比方說，如果您載入或卸載語音辨識文法，辨識器正在處理輸入時，使用這個方法和<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>辨識器的狀態與同步處理您的應用程式行為的事件。  
  
 辨識器呼叫這個方法時，會暫停或完成非同步作業，並會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件。 A<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件處理常式然後可以修改辨識作業之間的辨識器的狀態。 當處理<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件，辨識器暫停，直到傳回的事件處理常式。  
  
> [!NOTE]
>  如果辨識器的輸入變更辨識器引發之前<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件時，要求會被捨棄。  
  
 當呼叫這個方法：  
  
-   如果辨識器未處理的輸入，辨識器立即產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件。  
  
-   如果辨識器正在處理包含無回應或背景雜音的輸入，辨識器暫停辨識作業，並產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件。  
  
-   如果辨識器正在處理不是組成無回應或背景雜訊的輸入，辨識器完成辨識作業，並接著會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件。  
  
 當辨識器正在處理<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件：  
  
-   辨識器不會處理輸入，以及值<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>屬性維持不變。  
  
-   辨識器會繼續收集輸入的值<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>屬性可以變更。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>要求辨識器暫停以更新其狀態。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 當辨識器會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件，<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>屬性<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs>是`null`。  
  
 若要提供的使用者語彙基元，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>方法。 若要指定的音訊位置的位移，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>方法。  
  
   
  
## Examples  
 下列範例示範的主控台應用程式載入和卸載<xref:System.Speech.Recognition.Grammar>物件。 應用程式使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>方法來要求語音辨識引擎暫停讓它可以接收更新。 應用程式，然後載入或卸載<xref:System.Speech.Recognition.Grammar>物件。  
  
 在每個更新的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件會寫入目前載入的狀態與名稱<xref:System.Speech.Recognition.Grammar>至主控台的物件。 當載入和卸載的文法，應用程式第一次會辨識陣列動物的名稱和伺服器陣列的動物名稱的水果、 名稱然後水果的名稱。  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">使用者定義的資訊，包含該作業的資訊。</param>
        <summary>要求辨識器暫停以更新其狀態，並提供相關聯事件的使用者語彙基元。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 當辨識器會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件，<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>屬性<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs>包含的值`userToken`參數。  
  
 若要指定的音訊位置的位移，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>方法。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">使用者定義的資訊，包含該作業的資訊。</param>
        <param name="audioPositionAheadToRaiseUpdate">從目前的 <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> 起將要求延遲的位移。</param>
        <summary>要求辨識器暫停以更新其狀態，並提供相關聯事件的位移和使用者語彙基元。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 辨識器，不會啟動在辨識器的更新要求辨識器直到<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>等於目前<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>加上`audioPositionAheadToRaiseUpdate`。  
  
 當辨識器會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>事件，<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A>屬性<xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs>包含的值`userToken`參數。  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">音訊輸入資料流。</param>
        <param name="audioFormat">音訊輸入的格式。</param>
        <summary>設定<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />物件，以接收來自音訊資料流的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果辨識器已達到輸入資料流結尾辨識作業期間，辨識作業完成的可用的輸入。 任何後續的辨識作業會產生例外狀況，除非您更新辨識器的輸入。  
  
   
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 此範例會使用輸入音訊檔案，example.wav 包含片語，從 「 測試測試其中一個兩個三個"和"先生 cooper 」，以暫停分隔。 這個範例會產生下列輸出。  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>設定<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />物件，以接收來自預設音訊裝置的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 下列範例示範基本的語音辨識的主控台應用程式的一部分。 此範例會使用來自預設音訊裝置時，會執行多個非同步辨識作業並結束時使用者 utters 片語，"exit"。  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>停用語音辨識器的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 設定<xref:System.Speech.Recognition.SpeechRecognitionEngine>沒有輸入時使用的物件<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法，或取得辨識引擎暫時離線時。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">要做為輸入使用的檔案路徑。</param>
        <summary>設定<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />物件，以接收來自波形音訊格式 (.wav) 檔案的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果辨識器已達到輸入的檔案結尾辨識作業期間，辨識作業完成的可用的輸入。 任何後續的辨識作業會產生例外狀況，除非您更新辨識器的輸入。  
  
   
  
## Examples  
 下列範例會辨識執行.wav 檔案中的音效，並寫入主控台中的已辨識的文字。  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">包含音訊資料的資料流。</param>
        <summary>設定<see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />物件，以接收來自包含波形音訊格式 (.wav) 資料的資料流的輸入。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果辨識器已達到輸入資料流結尾辨識作業期間，辨識作業完成的可用的輸入。 任何後續的辨識作業會產生例外狀況，除非您更新辨識器的輸入。  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>當 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 偵測到可辨識為語音的輸入時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 每個語音辨識器有演算法，可區別無回應和語音。 當<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行語音辨識作業，便會產生<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>時其演算法識別為語音的輸入事件。 <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A>相關聯的屬性<xref:System.Speech.Recognition.SpeechDetectedEventArgs>物件表示在辨識器偵測到的語音位置的輸入資料流中的位置。 <xref:System.Speech.Recognition.SpeechRecognitionEngine>引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>才會產生任何事件<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>，或<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>事件。  
  
 如需詳細資訊，請參閱<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例是主控台應用程式選擇的班機的來源和目的地城市的一部分。 應用程式會辨識片語，例如 「 我想要從邁阿密飛抵芝加哥 」。  此範例會使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>事件以報告<xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>偵測到每個時間語音。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>在 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 已辨識的文字可能是文法中多個完整片語的元件時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine>會產生許多<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>事件，因為它會嘗試識別輸入的片語。 您可以存取中的部分已辨識片語的文字<xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>的屬性<xref:System.Speech.Recognition.SpeechHypothesizedEventArgs>中的處理常式物件<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>事件。 一般而言，處理這些事件是只用於偵錯。  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> 是衍生自 <xref:System.Speech.Recognition.RecognitionEventArgs>。  
  
 如需詳細資訊，請參閱<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性和<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>方法。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例會辨識片語，例如 「 顯示爵士的類別目錄中的演出者的清單 」。 此範例會使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>事件，以在主控台顯示不完整的片語片段，因為它們會被辨識。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>在 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 收到的輸入不符合任何其已載入且已啟用 <see cref="T:System.Speech.Recognition.Grammar" /> 物件時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 辨識器引發這個事件，如果它決定，輸入不符合足夠的信心與任何其已載入且已啟用<xref:System.Speech.Recognition.Grammar>物件。 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>的屬性<xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>包含 已拒絕<xref:System.Speech.Recognition.RecognitionResult>物件。 您可以使用的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>事件，以擷取辨識<xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>而遭到拒絕，且其<xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A>分數。  
  
 如果您的應用程式使用<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體，您可以修改在哪一個語音輸入接受或拒絕使用其中一個的信賴等級<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>方法。 您可以修改非語音輸入使用語音辨識的回應方式<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例會辨識片語這類 「 爵士的類別目錄中顯示演出者的清單 」 或 「 顯示專輯 gospel 」。 此範例會使用的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>語音輸入無法對應到具有足夠的文法的內容時，請在主控台中顯示通知的事件<xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A>產生成功辨識。 處理常式也會顯示辨識結果<xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>，遭到拒絕因為低的信心分數。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>在 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 收到的輸入符合任何其已載入且已啟用 <see cref="T:System.Speech.Recognition.Grammar" /> 物件時引發。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 您可以起始辨識作業使用其中一個<xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>或<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>方法。 辨識器會引發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件，如果它決定輸入符合其載入其中一個項目<xref:System.Speech.Recognition.Grammar>具有足夠構成辨識的信心層級的物件。 <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>的屬性<xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs>包含接受<xref:System.Speech.Recognition.RecognitionResult>物件。 處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件可以取得已辨識的片語，以及一份辨識<xref:System.Speech.Recognition.RecognitionResult.Alternates%2A>與較低的信心分數。  
  
 如果您的應用程式使用<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體，您可以修改在哪一個語音輸入接受或拒絕使用其中一個的信賴等級<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>方法。  您可以修改非語音輸入使用語音辨識的回應方式<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
 當辨識器接收到符合的文法的輸入<xref:System.Speech.Recognition.Grammar>物件可以引發其<xref:System.Speech.Recognition.Grammar.SpeechRecognized>事件。 <xref:System.Speech.Recognition.Grammar>物件的<xref:System.Speech.Recognition.Grammar.SpeechRecognized>就會引發事件之前的語音辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。 任何特定的文法的特定的工作應該一律執行的處理常式所<xref:System.Speech.Recognition.Grammar.SpeechRecognized>事件。  
  
 建立 <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> 委派時，必須識別處理事件的方法。 若要使事件與您的事件處理常式產生關聯，請將委派的執行個體 (Instance) 加入至事件。 除非您移除委派，否則每當事件發生時就會呼叫事件處理常式。 如需有關事件處理常式委派的詳細資訊，請參閱[事件與委派](https://go.microsoft.com/fwlink/?LinkId=162418)。  
  
   
  
## Examples  
 下列範例是主控台應用程式建立語音辨識文法，建構的一部分<xref:System.Speech.Recognition.Grammar>物件，並將資料載入<xref:System.Speech.Recognition.SpeechRecognitionEngine>來執行辨識。 此範例會示範語音輸入<xref:System.Speech.Recognition.SpeechRecognitionEngine>，相關聯的辨識結果，以及關聯的語音辨識器所引發的事件。  
  
 語音輸入，例如 「 我想要從芝加哥飛到邁阿密 」 將會觸發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。 說到片語 「 飛我從休士頓對芝加哥 」 不會觸發<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件。  
  
 此範例會使用的處理常式<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>事件，以顯示已成功辨識片語和它們包含在主控台中的語意。  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>從辨識器卸載所有 <see cref="T:System.Speech.Recognition.Grammar" /> 物件。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果辨識器目前正在載入<xref:System.Speech.Recognition.Grammar>以非同步的方式，這個方法會等到<xref:System.Speech.Recognition.Grammar>載入時之前它會卸載所有,<xref:System.Speech.Recognition.Grammar>物件從<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體。  
  
 若要卸除特定的文法，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>方法。  
  
   
  
## Examples  
 下列範例示範同步載入和卸載的語音辨識文法的主控台應用程式的一部分。  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">要卸載的文法物件。</param>
        <summary>從 <see cref="T:System.Speech.Recognition.Grammar" /> 執行個體卸載指定的 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 物件。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 如果辨識器正在執行，應用程式必須使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>暫停<xref:System.Speech.Recognition.SpeechRecognitionEngine>執行個體，再載入、 卸載、 啟用，或停用<xref:System.Speech.Recognition.Grammar>物件。 若要卸載所有<xref:System.Speech.Recognition.Grammar>物件，使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A>方法。  
  
   
  
## Examples  
 下列範例示範同步載入和卸載的語音辨識文法的主控台應用程式的一部分。  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.InvalidOperationException">這個辨識器未將文法載入，或這個辨識器正在以非同步方式載入文法。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>更新辨識器的設定值。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 辨識器的設定可以包含字串、 64 位元整數或記憶體位址資料。 下表描述所定義的 Microsoft Speech API (SAPI) 設定為符合規範的辨識器。 下列設定必須有相同的範圍，支援設定每個辨識器。 SAPI 相容的辨識器不需要支援這些設定，而且可支援其他設定。  
  
|名稱|描述|  
|----------|-----------------|  
|`ResourceUsage`|指定辨識器的 CPU 耗用量。 範圍是從 0 到 100 之間。 預設值為 50。|  
|`ResponseSpeed`|語音辨識器完成辨識作業之前，請指定模稜兩可的輸入結尾處的無回應的長度。 範圍是從 0 到 10000 毫秒 (ms)。 此設定會對應至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>屬性。 預設 = 150ms年。|  
|`ComplexResponseSpeed`|語音辨識器完成辨識作業之前，請模稜兩可的輸入結尾指出無回應，以毫秒為單位 (ms) 的長度。 範圍是從 0 到 10,000。 此設定會對應至辨識器<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。 預設 = 500 毫秒。|  
|`AdaptationOn`|指出調適的原音模型是否為 ON (值 = `1`) 或 OFF (值 = `0`)。 預設值是`1`(ON)。|  
|`PersistedBackgroundAdaptation`|指出背景適應是否為 ON (值 = `1`) 或 OFF (值 = `0`)，並保存在登錄中的設定。 預設值是`1`(ON)。|  
  
 若要傳回其中一個辨識器的設定，請使用<xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A>方法。  
  
 除了`PersistedBackgroundAdaptation`，設定使用的屬性值<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>方法的有效日期持續僅針對目前的執行個體<xref:System.Speech.Recognition.SpeechRecognitionEngine>之後將它們還原為預設值。  
  
 您可以修改非語音輸入使用語音辨識的回應方式<xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>， <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>，和<xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A>屬性。  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">要更新之設定的名稱。</param>
        <param name="updatedValue">設定的新值。</param>
        <summary>以指定的整數值更新 <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> 的指定設定。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 除了`PersistedBackgroundAdaptation`，設定使用的屬性值<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>方法的有效日期持續僅針對目前的執行個體<xref:System.Speech.Recognition.SpeechRecognitionEngine>之後將它們還原為預設值。 請參閱<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>如需支援的設定的描述。  
  
   
  
## Examples  
 下列範例是主控台應用程式，讓它輸出幾個的辨識器支援 EN-US 地區設定所定義的設定值的一部分。 此範例更新信心層級設定，並接著會查詢要檢查之更新的值的辨識器。 這個範例會產生下列輸出。  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> 為空字串 ("")。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">辨識器沒有該名稱的設定。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">要更新之設定的名稱。</param>
        <param name="updatedValue">設定的新值。</param>
        <summary>以指定的字串值來更新指定的語音辨識引擎設定。</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 除了`PersistedBackgroundAdaptation`，設定使用的屬性值<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>方法的有效日期持續僅針對目前的執行個體<xref:System.Speech.Recognition.SpeechRecognitionEngine>之後將它們還原為預設值。 請參閱<xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A>如需支援的設定的描述。  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> 為 <see langword="null" />。</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> 為空字串 ("")。</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">辨識器沒有該名稱的設定。</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>